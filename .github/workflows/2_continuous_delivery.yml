name: 2 - Continuous Delivery
run-name: "${{ github.workflow }}"

permissions: write-all

on:
  # Runs when the previous stage finished
  workflow_run:
    workflows: ["1 - Continuous Integration"]
    types: [completed]

  workflow_dispatch:

env:
  REGISTRY: ${{ vars.REGISTRY }}
  MLFLOW_TRACKING_URI: ${{ vars.MLFLOW_TRACKING_URI }}
  MLFLOW_EXPERIMENT_NAME: ${{ vars.MLFLOW_EXPERIMENT_NAME }}
  MLFLOW_MODEL_NAME: ${{ vars.MLFLOW_MODEL_NAME }}

jobs:
  train-model:
    # Run only if previous stage finished successfully
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.13" }
      - run: pip install uv
      - run: uv sync

      # Log in to GitHub Container Registry (GHCR)
      # We will get the serving image with tag staging from here to validate
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Download models image built in the previous stage
      - run: docker pull ${{ env.REGISTRY }}/${{ github.repository }}/models:${{ github.sha }}

      # Download TLC dataset for years 2011, 2012, 2013
      - name: Download TLC dataset (2011-2013)
        env:
          DATA_PATH: ${{ env.DATA_PATH }}
        run: |
          # Use DATA_PATH to store data
          mkdir -p $DATA_PATH/2011 $DATA_PATH/2012 $DATA_PATH/2013
          base="https://d37ci6vzurychx.cloudfront.net/trip-data"

          for year in 2011 2012 2013; do
            for month in {01..12}; do
              file="yellow_tripdata_${year}-${month}.parquet"
              url="$base/$file"
              echo "Downloading $file"
              curl -SL "$url" -o "$DATA_PATH/${year}/$file"
            done
          done

      # Run models container which runs the models script within
      # The models script will select the "best" model and tag it with the sha of the commit in mlflow
      - name: Run models
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_EXPERIMENT_NAME: ${{ env.MLFLOW_EXPERIMENT_NAME }}
          MLFLOW_MODEL_NAME: ${{ env.MLFLOW_MODEL_NAME }}
          COMMIT_SHA: ${{ github.sha }}
          DATA_PATH: ${{ env.DATA_PATH }}
        run: |
          docker run --rm \
            -v ${{ env.DATA_PATH }}:/app/data \
            -e MLFLOW_TRACKING_URI \
            -e MLFLOW_MODEL_NAME \
            -e MLFLOW_EXPERIMENT_NAME \
            -e COMMIT_SHA \
            ${{ env.REGISTRY }}/${{ github.repository }}/models:${{ github.sha }}

      # Download serving image built in the previous stage
      - run: docker pull ${{ env.REGISTRY }}/${{ github.repository }}/serving:${{ github.sha }}

      # Start serving image
      # It is configured to load the model passed in env variable MLFLOW_MODEL_NAME and alias MODEL_ALIAS
      - name: Start serving container
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MODEL_ALIAS: ${{ github.sha }}
          MLFLOW_MODEL_NAME: ${{ env.MLFLOW_MODEL_NAME }}
        run: |
          docker run -d \
            --name serving-app \
            -p 8080:8080 \
            -e MLFLOW_TRACKING_URI \
            -e MODEL_ALIAS \
            -e MLFLOW_MODEL_NAME \
            ${{ env.REGISTRY }}/${{ github.repository }}/serving:${{ github.sha }}

          # Wait for FastAPI to be ready
          sleep 10

      # Run end to end tests, this will test the serving app to see if it is working ok
      # Additional metrics and verification can assess performance metrics as well
      - name: Run Staging tests
        run: uv run pytest tests/test_e2e.py -v

      - name: Show container logs on failure
        if: failure()
        run: |
          echo "=== FastAPI logs ==="
          docker logs serving-app || true

      # Log in to GitHub Container Registry (GHCR)
      # This is the repository where we will upload our images once they have been validated
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Tag serving image with staging; this image will be used for further testing in staging
      # Push serving image to repository
      - name: Push serving staging image
        run: |
          docker tag ${{ env.REGISTRY }}/${{ github.repository }}/serving:${{ github.sha }} \
                     ${{ env.REGISTRY }}/${{ github.repository }}/serving:staging
          docker push ${{ env.REGISTRY }}/${{ github.repository }}/serving:staging

      # Promote selected model to staging to be further validated
      # The script will look for the MLFLOW_MODEL_NAME and promote it from FROM_ALIAS to TO_ALIAS
      - name: Promote model to staging
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_EXPERIMENT_NAME: ${{ env.MLFLOW_EXPERIMENT_NAME }}
          MLFLOW_MODEL_NAME: ${{ env.MLFLOW_MODEL_NAME }}
          FROM_ALIAS: ${{ github.sha }}
          TO_ALIAS: staging
        run: uv run python model-promotion/promote_model.py
